---
title: Overview
description: Cortex Overview
slug: "basic-usage"
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex.cpp is currently under development. Our documentation outlines the intended behavior of Cortex, which may not yet be fully implemented in the codebase.
:::

Cortex has an [API server](https://cortex.so/api-reference) that runs at `localhost:39281`.


## Usage
### Start Cortex.cpp Server
<Tabs>
  <TabItem value="MacOs/Linux" label="MacOs/Linux">
  ```sh
  # Stable
  cortex start

  # Beta
  cortex-beta start

  # Nightly
  cortex-nightly start
  ```
  </TabItem>
  <TabItem value="Windows" label="Windows">
  ```sh
  # Stable
  cortex.exe start

  # Beta
  cortex-beta.exe start

  # Nightly
  cortex-nightly.exe start
  ```
  </TabItem>
</Tabs>
### Run Model
```bash
# Pull a model
curl --request POST \
  --url http://localhost:39281/v1/models/pull \
  --header 'Content-Type: application/json' \
  --data '{
    "model": "mistral:gguf"
}' 
# Start the model
curl --request POST \
  --url http://localhost:39281/v1/models/start \
  --header 'Content-Type: application/json' \
  --data '{
  "model": "mistral:gguf"
  "prompt_template": "system\n{system_message}\nuser\n{prompt}\nassistant",
  "stop": [],
  "ngl": 4096,
  "ctx_len": 4096,
  "cpu_threads": 10,
  "n_batch": 2048,
  "caching_enabled": true,
  "grp_attn_n": 1,
  "grp_attn_w": 512,
  "mlock": false,
  "flash_attn": true,
  "cache_type": "f16",
  "use_mmap": true,
  "engine": "llama-cpp"
}'
```
### Chat with Model
```bash
# Invoke the chat completions endpoint
curl http://localhost:39281/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    },
  ],
  "model": "mistral:gguf",
  "stream": true,
  "max_tokens": 1,
  "stop": [
      null
  ],
  "frequency_penalty": 1,
  "presence_penalty": 1,
  "temperature": 1,
  "top_p": 1
}'
```
### Stop Model
```bash
# Stop a model
curl --request POST \
  --url http://localhost:39281/v1/models/stop \
  --header 'Content-Type: application/json' \
  --data '{
    "model": "mistral:gguf"
}' 
```
### Stop Cortex.cpp Server
<Tabs>
  <TabItem value="MacOs/Linux" label="MacOs/Linux">
  ```sh
  # Stable
  cortex stop

  # Beta
  cortex-beta stop

  # Nightly
  cortex-nightly stop
  ```
  </TabItem>
  <TabItem value="Windows" label="Windows">
  ```sh
  # Stable
  cortex.exe stop

  # Beta
  cortex-beta.exe stop

  # Nightly
  cortex-nightly.exe stop
  ```
  </TabItem>
</Tabs>