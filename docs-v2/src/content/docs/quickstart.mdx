---
title: Quickstart
description: Cortex Quickstart.
slug: quickstart
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";


## Local Installation

Cortex has a **Local Installer** with all of the required dependencies, so that once you've downloaded it, no internet connection is required during the installation process.
- [Windows](https://app.cortexcpp.com/download/latest/windows-amd64-local)
- [Mac (Universal)](https://app.cortexcpp.com/download/latest/mac-universal-local)
- [Linux](https://app.cortexcpp.com/download/latest/linux-amd64-local)

## Start a Cortex Server

This command starts the Cortex's' API server at `localhost:39281`.
<Tabs>
  <TabItem value="MacOs/Linux" label="MacOs/Linux">
  ```sh
  cortex start
  ```
  </TabItem>
  <TabItem value="Windows" label="Windows">
  ```sh
  cortex.exe start
  ```
  </TabItem>
</Tabs>

## Pull Models

This command allows users to download a model from these Model Hubs:
- [Cortex Built-in Models](https://cortex.so/models)
- [Hugging Face](https://huggingface.co) (GGUF): `cortex pull <author/ModelRepo>`

It displays available quantizations, recommends a default and downloads the desired quantization.

<Tabs>
  <TabItem value="MacOs/Linux" label="MacOs/Linux">
  The following two options will show you all of the available models under those names. Cortex will first search
  on its own hub for models like `llama3.3`, and in huggingface for hyper specific ones like `bartowski/Meta-Llama-3.1-8B-Instruct-GGU`.
  ```sh
  cortex pull llama3.3
  ```
  or,

  ```sh
  cortex pull bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
  ```
  </TabItem>
  <TabItem value="Windows" label="Windows">
  ```sh
  cortex pull llama3.3
  ```
  ```sh
  cortex.exe pull bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
  ```
  </TabItem>
</Tabs>

## Run a Model

This command downloads the default `gguf` model (if not available in your file system) from the
[Cortex Hub](https://huggingface.co/cortexso), starts the model, and chat with the model.

<Tabs>
  <TabItem value="MacOs/Linux" label="MacOs/Linux">
  ```sh
  cortex run llama3.3
  ```
  </TabItem>
  <TabItem value="Windows" label="Windows">
  ```sh
  cortex.exe run llama3.3
  ```
  </TabItem>
</Tabs>

:::info
All model files are stored in the `~/cortex/models` folder.
:::

## Using the Model

### API
```sh
curl http://localhost:39281/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "llama3.1:8b-gguf",
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    },
  ],
  "stream": true,
  "max_tokens": 1,
  "stop": [
      null
  ],
  "frequency_penalty": 1,
  "presence_penalty": 1,
  "temperature": 1,
  "top_p": 1
}'
```
Refer to our [API documentation](https://cortex.so/api-reference) for more details.

## Show the System State

This command displays the running model and the hardware system status (RAM, Engine, VRAM, Uptime).

<Tabs>
  <TabItem value="MacOs/Linux" label="MacOs/Linux">
  ```sh
  cortex ps
  ```
  </TabItem>
  <TabItem value="Windows" label="Windows">
  ```sh
  cortex.exe ps
  ```
  </TabItem>
</Tabs>

## Stop a Model

This command stops the running model.

<Tabs>
  <TabItem value="MacOs/Linux" label="MacOs/Linux">
  ```sh
  cortex models stop llama3.3
  ```
  </TabItem>
  <TabItem value="Windows" label="Windows">
  ```sh
  cortex.exe models stop llama3.3
  ```
  </TabItem>
</Tabs>

## Stop a Cortex Server

This command stops the Cortex.cpp API server at `localhost:39281` or whichever other port you used to start cortex.

<Tabs>
  <TabItem value="MacOs/Linux" label="MacOs/Linux">
  ```sh
  cortex stop
  ```
  </TabItem>
  <TabItem value="Windows" label="Windows">
  ```sh
  cortex.exe stop
  ```
  </TabItem>
</Tabs>

## What's Next?
Now that Cortex is set up, you can continue on to any of the following sections:

- Adjust the folder path and configuration using the [`.cortexrc`](/docs/architecture/cortexrc) file.
- Explore the Cortex's [data folder](/docs/architecture/data-folder) to understand how data gets stored.
- Learn about the structure of the [`model.yaml`](/docs/capabilities/models/model-yaml) file in Cortex.
